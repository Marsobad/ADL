{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_ppi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN7kg8srxNhuCg4wDR+odgh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marsobad/ADL/blob/master/Train_ppi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkfLwJyenfO1",
        "colab_type": "code",
        "outputId": "6ec01ac9-9318-4a36-cb33-f9bbc9004c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install dgl-cu101"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dgl-cu101 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl-cu101) (2.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl-cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl-cu101) (1.17.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl-cu101) (4.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSsqBZiemigk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl import batch\n",
        "from dgl.data.ppi import LegacyPPIDataset\n",
        "from dgl.nn.pytorch import GraphConv\n",
        "from dgl.nn.pytorch import edge_softmax, GATConv\n",
        "from sklearn.metrics import f1_score\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aOtY5Lhmncf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MODEL_STATE_FILE = path.join(path.dirname(path.abspath(__file__)), \"model_state.pth\")\n",
        "#in_channels, out_channels, heads=1, concat=True, negative_slope=0.2, dropout=0, bias=True\n",
        "MODEL_STATE_FILE = \"model_state.pth\"\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, g, num_layers, in_dim, num_hidden, num_classes, heads, feat_drop, attn_drop, negative_slope, residual, activation):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        self.g = g\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATConv(in_dim, num_hidden, heads, feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATConv(num_hidden * heads, num_hidden, heads, feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATConv(num_hidden * heads, num_classes, heads, feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = inputs\n",
        "        for l in range(len(self.gat_layers)- 1):\n",
        "            h = self.gat_layers[l](self.g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.gat_layers[-1](self.g, h).mean(1)\n",
        "        return logits\n",
        "\n",
        "class BasicGraphModel(nn.Module):\n",
        "\n",
        "    def __init__(self, g, n_layers, input_size, hidden_size, output_size, nonlinearity):\n",
        "        super().__init__()\n",
        "\n",
        "        self.g = g\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(GraphConv(input_size, hidden_size, activation=nonlinearity))\n",
        "        for i in range(n_layers - 1):\n",
        "            self.layers.append(GraphConv(hidden_size, hidden_size, activation=nonlinearity))\n",
        "        self.layers.append(GraphConv(hidden_size, output_size))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = inputs\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            outputs = layer(self.g, outputs)\n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyEM1T8m9lye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(args):\n",
        "    # create the dataset\n",
        "    train_dataset, test_dataset = LegacyPPIDataset(mode=\"train\"), LegacyPPIDataset(mode=\"test\")\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=collate_fn)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn)\n",
        "    n_features, n_classes = train_dataset.features.shape[1], train_dataset.labels.shape[1]\n",
        "\n",
        "    #g, features, labels, mask = load_cora_data()\n",
        "    # create the model, loss function and optimizer\n",
        "    device = torch.device(\"cpu\" if args.gpu < 0 else \"cuda:\" + str(args.gpu))\n",
        "    #( g, num_layers, in_dim, num_hidden, num_classes, heads, activation, feat_drop, attn_drop, negative_slope, residual)\n",
        "    model = GAT(g=train_dataset.graph, num_layers=3, in_dim=n_features, num_hidden=256, num_classes=n_classes, heads=10, feat_drop=0., attn_drop=0., negative_slope=0.2, residual=False, activation= F.elu).to(device)\n",
        "    #model = BasicGraphModel(g=train_dataset.graph, n_layers=2, input_size=n_features,hidden_size=256, output_size=n_classes, nonlinearity=F.elu).to(device)\n",
        "    loss_fcn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    #optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "    # train and test\n",
        "    if args.mode == \"train\":\n",
        "        train(model, loss_fcn, device, optimizer, train_dataloader, test_dataset)\n",
        "        torch.save(model.state_dict(), MODEL_STATE_FILE)\n",
        "    model.load_state_dict(torch.load(MODEL_STATE_FILE))\n",
        "    return test(model, loss_fcn, device, test_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4fY8G45f2Y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_fcn, device, optimizer, train_dataloader, test_dataset):\n",
        "  fones = []\n",
        "  for epoch in range(args.epochs):\n",
        "      model.train()\n",
        "      losses = []\n",
        "      for batch, data in enumerate(train_dataloader):\n",
        "          subgraph, features, labels = data\n",
        "          features = features.to(device)\n",
        "          labels = labels.to(device)\n",
        "          model.g = subgraph\n",
        "          for layer in model.gat_layers:\n",
        "              layer.g = subgraph\n",
        "          logits = model(features.float())\n",
        "          loss = loss_fcn(logits, labels.float())\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.item())\n",
        "      loss_data = np.array(losses).mean()\n",
        "      print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
        "\n",
        "      if epoch % 5 == 0:\n",
        "          scores = []\n",
        "          fones_mean = []\n",
        "          for batch, test_data in enumerate(test_dataset):\n",
        "              subgraph, features, labels = test_data\n",
        "              features = torch.tensor(features).to(device)\n",
        "              labels = torch.tensor(labels).to(device)\n",
        "              score, _ = evaluate(features.float(), model, subgraph, labels.float(), loss_fcn)\n",
        "              scores.append(score)\n",
        "              fones_mean.append(score)\n",
        "          print(\"F1-Score: {:.4f} \".format(np.array(scores).mean()))\n",
        "          fones.append(np.array(fones_mean).mean())\n",
        "  print(score)\n",
        "  #plot the training error wrt. the number of epochs: \n",
        "  plt.plot(range(1,251, 5), fones)\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.ylabel(\"F1 score\")\n",
        "  plt.title(\"Visualization of F1 score\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu8tee9Hf40b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, loss_fcn, device, test_dataloader):\n",
        "    test_scores = []\n",
        "    for batch, test_data in enumerate(test_dataloader):\n",
        "        subgraph, features, labels = test_data\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "        test_scores.append(evaluate(features, model, subgraph, labels.float(), loss_fcn)[0])\n",
        "    mean_scores = np.array(test_scores).mean()\n",
        "    print(\"F1-Score: {:.4f}\".format(np.array(test_scores).mean()))\n",
        "    return mean_scores\n",
        "\n",
        "\n",
        "def evaluate(features, model, subgraph, labels, loss_fcn):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        model.g = subgraph\n",
        "        for layer in model.gat_layers:\n",
        "            layer.g = subgraph\n",
        "        output = model(features.float())\n",
        "        loss_data = loss_fcn(output, labels.float())\n",
        "        predict = np.where(output.data.cpu().numpy() >= 0.5, 1, 0)\n",
        "        score = f1_score(labels.data.cpu().numpy(), predict, average=\"micro\")\n",
        "        return score, loss_data.item()\n",
        "\n",
        "\n",
        "def collate_fn(sample):\n",
        "    graphs, features, labels = map(list, zip(*sample))\n",
        "    graph = batch(graphs)\n",
        "    features = torch.from_numpy(np.concatenate(features))\n",
        "    labels = torch.from_numpy(np.concatenate(labels))\n",
        "    return graph, features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7-_cQgIf59z",
        "colab_type": "code",
        "outputId": "6e179754-c0fe-4dcc-d602-645a5494b314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--mode\",  choices=[\"train\", \"test\"], default=\"train\")\n",
        "parser.add_argument(\"--gpu\", type=int, default=-1, help=\"GPU to use. Set -1 to use CPU.\")\n",
        "parser.add_argument(\"--epochs\", type=int, default=250)\n",
        "parser.add_argument(\"--batch-size\", type=int, default=2)\n",
        "args = parser.parse_args([\"--mode\",\"train\", \"--gpu\",\"0\",\"--epochs\",\"250\", \"--batch-size\", \"2\"])\n",
        "main(args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading G...\n",
            "Loading G...\n",
            "Epoch 00001 | Loss: 0.5987\n",
            "F1-Score: 0.3433 \n",
            "Epoch 00002 | Loss: 0.5611\n",
            "Epoch 00003 | Loss: 0.5420\n",
            "Epoch 00004 | Loss: 0.5322\n",
            "Epoch 00005 | Loss: 0.5260\n",
            "Epoch 00006 | Loss: 0.5210\n",
            "F1-Score: 0.3644 \n",
            "Epoch 00007 | Loss: 0.5194\n",
            "Epoch 00008 | Loss: 0.5154\n",
            "Epoch 00009 | Loss: 0.5102\n",
            "Epoch 00010 | Loss: 0.5101\n",
            "Epoch 00011 | Loss: 0.5139\n",
            "F1-Score: 0.3487 \n",
            "Epoch 00012 | Loss: 0.5031\n",
            "Epoch 00013 | Loss: 0.4964\n",
            "Epoch 00014 | Loss: 0.4898\n",
            "Epoch 00015 | Loss: 0.4912\n",
            "Epoch 00016 | Loss: 0.4940\n",
            "F1-Score: 0.3212 \n",
            "Epoch 00017 | Loss: 0.4833\n",
            "Epoch 00018 | Loss: 0.4734\n",
            "Epoch 00019 | Loss: 0.4652\n",
            "Epoch 00020 | Loss: 0.4562\n",
            "Epoch 00021 | Loss: 0.4498\n",
            "F1-Score: 0.4996 \n",
            "Epoch 00022 | Loss: 0.4466\n",
            "Epoch 00023 | Loss: 0.4430\n",
            "Epoch 00024 | Loss: 0.4468\n",
            "Epoch 00025 | Loss: 0.4501\n",
            "Epoch 00026 | Loss: 0.4375\n",
            "F1-Score: 0.5735 \n",
            "Epoch 00027 | Loss: 0.4245\n",
            "Epoch 00028 | Loss: 0.4077\n",
            "Epoch 00029 | Loss: 0.3953\n",
            "Epoch 00030 | Loss: 0.3848\n",
            "Epoch 00031 | Loss: 0.3756\n",
            "F1-Score: 0.6617 \n",
            "Epoch 00032 | Loss: 0.3718\n",
            "Epoch 00033 | Loss: 0.3761\n",
            "Epoch 00034 | Loss: 0.3768\n",
            "Epoch 00035 | Loss: 0.3764\n",
            "Epoch 00036 | Loss: 0.3677\n",
            "F1-Score: 0.6728 \n",
            "Epoch 00037 | Loss: 0.3536\n",
            "Epoch 00038 | Loss: 0.3374\n",
            "Epoch 00039 | Loss: 0.3247\n",
            "Epoch 00040 | Loss: 0.3158\n",
            "Epoch 00041 | Loss: 0.3126\n",
            "F1-Score: 0.6888 \n",
            "Epoch 00042 | Loss: 0.3118\n",
            "Epoch 00043 | Loss: 0.3220\n",
            "Epoch 00044 | Loss: 0.3094\n",
            "Epoch 00045 | Loss: 0.2918\n",
            "Epoch 00046 | Loss: 0.2752\n",
            "F1-Score: 0.7780 \n",
            "Epoch 00047 | Loss: 0.2668\n",
            "Epoch 00048 | Loss: 0.2603\n",
            "Epoch 00049 | Loss: 0.2594\n",
            "Epoch 00050 | Loss: 0.2574\n",
            "Epoch 00051 | Loss: 0.2596\n",
            "F1-Score: 0.8109 \n",
            "Epoch 00052 | Loss: 0.2561\n",
            "Epoch 00053 | Loss: 0.2480\n",
            "Epoch 00054 | Loss: 0.2344\n",
            "Epoch 00055 | Loss: 0.2215\n",
            "Epoch 00056 | Loss: 0.2119\n",
            "F1-Score: 0.8189 \n",
            "Epoch 00057 | Loss: 0.2088\n",
            "Epoch 00058 | Loss: 0.2066\n",
            "Epoch 00059 | Loss: 0.2062\n",
            "Epoch 00060 | Loss: 0.2079\n",
            "Epoch 00061 | Loss: 0.2095\n",
            "F1-Score: 0.8481 \n",
            "Epoch 00062 | Loss: 0.1990\n",
            "Epoch 00063 | Loss: 0.1884\n",
            "Epoch 00064 | Loss: 0.1784\n",
            "Epoch 00065 | Loss: 0.1701\n",
            "Epoch 00066 | Loss: 0.1652\n",
            "F1-Score: 0.8695 \n",
            "Epoch 00067 | Loss: 0.1620\n",
            "Epoch 00068 | Loss: 0.1614\n",
            "Epoch 00069 | Loss: 0.1650\n",
            "Epoch 00070 | Loss: 0.1678\n",
            "Epoch 00071 | Loss: 0.1717\n",
            "F1-Score: 0.8395 \n",
            "Epoch 00072 | Loss: 0.1704\n",
            "Epoch 00073 | Loss: 0.1540\n",
            "Epoch 00074 | Loss: 0.1454\n",
            "Epoch 00075 | Loss: 0.1371\n",
            "Epoch 00076 | Loss: 0.1321\n",
            "F1-Score: 0.8649 \n",
            "Epoch 00077 | Loss: 0.1280\n",
            "Epoch 00078 | Loss: 0.1246\n",
            "Epoch 00079 | Loss: 0.1241\n",
            "Epoch 00080 | Loss: 0.1207\n",
            "Epoch 00081 | Loss: 0.1231\n",
            "F1-Score: 0.8768 \n",
            "Epoch 00082 | Loss: 0.1263\n",
            "Epoch 00083 | Loss: 0.1245\n",
            "Epoch 00084 | Loss: 0.1247\n",
            "Epoch 00085 | Loss: 0.1229\n",
            "Epoch 00086 | Loss: 0.1219\n",
            "F1-Score: 0.8890 \n",
            "Epoch 00087 | Loss: 0.1170\n",
            "Epoch 00088 | Loss: 0.1191\n",
            "Epoch 00089 | Loss: 0.1151\n",
            "Epoch 00090 | Loss: 0.1147\n",
            "Epoch 00091 | Loss: 0.1104\n",
            "F1-Score: 0.9147 \n",
            "Epoch 00092 | Loss: 0.1126\n",
            "Epoch 00093 | Loss: 0.1087\n",
            "Epoch 00094 | Loss: 0.1068\n",
            "Epoch 00095 | Loss: 0.1054\n",
            "Epoch 00096 | Loss: 0.1026\n",
            "F1-Score: 0.9137 \n",
            "Epoch 00097 | Loss: 0.1058\n",
            "Epoch 00098 | Loss: 0.1074\n",
            "Epoch 00099 | Loss: 0.1101\n",
            "Epoch 00100 | Loss: 0.1121\n",
            "Epoch 00101 | Loss: 0.1154\n",
            "F1-Score: 0.9092 \n",
            "Epoch 00102 | Loss: 0.1176\n",
            "Epoch 00103 | Loss: 0.1155\n",
            "Epoch 00104 | Loss: 0.1198\n",
            "Epoch 00105 | Loss: 0.1166\n",
            "Epoch 00106 | Loss: 0.1123\n",
            "F1-Score: 0.8787 \n",
            "Epoch 00107 | Loss: 0.1053\n",
            "Epoch 00108 | Loss: 0.0982\n",
            "Epoch 00109 | Loss: 0.0966\n",
            "Epoch 00110 | Loss: 0.0962\n",
            "Epoch 00111 | Loss: 0.0952\n",
            "F1-Score: 0.9210 \n",
            "Epoch 00112 | Loss: 0.0881\n",
            "Epoch 00113 | Loss: 0.0841\n",
            "Epoch 00114 | Loss: 0.0836\n",
            "Epoch 00115 | Loss: 0.0777\n",
            "Epoch 00116 | Loss: 0.0692\n",
            "F1-Score: 0.9409 \n",
            "Epoch 00117 | Loss: 0.0681\n",
            "Epoch 00118 | Loss: 0.0615\n",
            "Epoch 00119 | Loss: 0.0586\n",
            "Epoch 00120 | Loss: 0.0564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKreuRhthdt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}